{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 구조적 API 개요\n",
    "\n",
    "- 배치와 스트리밍처리에서 구조적 API 활용 가능\n",
    "- 구조적 API를 통해 배치 작업을 스트리밍 작업으로 손쉽게 변환시킬 수 있음\n",
    "\n",
    "### 4.1 DataFrame과 Dataset\n",
    "\n",
    "- 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션\n",
    "- 어떤 데이터에 어떤 연산을 적용해야 하는지 정의하는 지연 연산의 실행 계획이고, 불변성을 가짐\n",
    "- 스키마는 분산 컬렉션에 저장할 데이터 타입을 정의하는 방법\n",
    "\n",
    "\n",
    "### 4.2 스키마\n",
    "- 스키마는 DataFrame의 컬럼명과 데이터 타입을 정의\n",
    "\n",
    "### 4.3 스파크의 구조적 데이터 타입 개요\n",
    "- 스파크는 실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 카탈리스트 엔진 사용\n",
    "- 카탈리스트를 통해 python, R 같은 프로그래밍 언어에서도 spark 언어로 처리됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(number + 10)|\n",
      "+-------------+\n",
      "|           10|\n",
      "|           11|\n",
      "|           12|\n",
      "|           13|\n",
      "|           14|\n",
      "|           15|\n",
      "|           16|\n",
      "|           17|\n",
      "|           18|\n",
      "|           19|\n",
      "|           20|\n",
      "|           21|\n",
      "|           22|\n",
      "|           23|\n",
      "|           24|\n",
      "|           25|\n",
      "|           26|\n",
      "|           27|\n",
      "|           28|\n",
      "|           29|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 스파크 덧셈연산\n",
    "df = spark.range(500).toDF('number')\n",
    "df.select(df['number'] + 10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 DataFrame과 Dataset 비교\n",
    "- DataFrame: '비타입형'\n",
    "  - 스키마에 명시된 데이터 타입 일치 여부를 런타임이 되어서야 확인 가능\n",
    "\n",
    "- Dataset: '타입형'\n",
    "  - 스키마에 명시된 데이터 타입 일치 여부를 컴파일 타임에 확인\n",
    "  - JVM 기반인 스칼라와 자바에서 지원\n",
    "\n",
    "- python에서는 Dataset 사용불가, 하지만 DataFrame으로 비슷하게 처리 가능\n",
    "\n",
    "### 4.3.2 컬럼\n",
    "- 단순 데이터 타입, 복합 데이터 타입, null 타입을 표현\n",
    "\n",
    "### 4.3.3 로우\n",
    "- 데이터 레코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 로우를 얻는 과정\n",
    "spark.range(2).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 스파크 데이터 타입\n",
    "- 여러 내부 데이터 타입을 가지고 있음\n",
    "\n",
    "### 4.4 구조적 API 실행 과정\n",
    "1. 코드작성\n",
    "2. 논리적 실행 계획 변환\n",
    "3. 논리적 실행 계획을 물리적 실행 계획으로 변환\n",
    "4. 스파크 클러스터에서 물리적 실행 계획 처리\n",
    "\n",
    "### 4.4.1 논리적 실행 계획\n",
    "\n",
    "사용자 코드 -> 검증 전 논리적 실행 계획 -> 분석(카탈로그) -> 검증된 논리적 실행 계획 -> 최적화된 논리적 실행 계획\n",
    "\n",
    "### 4.4.2 물리적 실행 계획\n",
    "- 논리적 실행 계획을 클러스터 환경에서 실행하는 방법 정의\n",
    "\n",
    "최적화된 논리적 실행 계획 -> 물리적 실행 계획 -> 비용 모델 -> 최적의 물리적 실행 -> 클러스터에서 처리\n",
    "\n",
    "- 스파크를 컴파일러라고 부름 -> DataFrame, Dataset, SQL로 정의된 쿼리를 RDD 트랜스포메이션으로 변환시켜줌. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
