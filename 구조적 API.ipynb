{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. 구조적 API 개요\n",
    "\n",
    "구조적 API에는 다음과 같은 세 가지 분산 컬렉션 API가 있다.\n",
    "- Dataset\n",
    "- DataFrame\n",
    "- SQL 테이블과 뷰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 DataFrame과 Dataset\n",
    "DataFrame과 Dataset은 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션이다. 각 컬럼은 다른 컬럼과 동일한 수의 로우를 가져야 한다(값이 없으면 null 처리). 컬렉션과 로우는 같은 데이터 타입 정보를 가지고 있어야 한다. 이 둘은 결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야 하는지 정의하는 지연 연산의 실행 계획이며 불변성을 가진다. \n",
    "\n",
    "## 4.2 스키마\n",
    "스키마는 데이터프레임의 컬럼명과 데이터 타입을 정의한다. 데이터소스에서 얻거나(스키마 온 리드라고 한다) 직접 정의할 수 있다. \n",
    "\n",
    "## 4.3 스파크의 구조적 데이터 타입 개요\n",
    "스파크는 실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 카탈리스트 엔진을 사용한다. 스파크 자체 데이터 타입을 지원하는 여러 언어 API와 직접 매핑되며, 각 언어에 대한 매핑 테이블을 가지고 있다. 파이썬을 이용해 구조적 API를 사용하더라도 대부분의 연산은 스파크의 데이터 타입을 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(number + 10)|\n",
      "+-------------+\n",
      "|           10|\n",
      "|           11|\n",
      "|           12|\n",
      "|           13|\n",
      "|           14|\n",
      "|           15|\n",
      "|           16|\n",
      "|           17|\n",
      "|           18|\n",
      "|           19|\n",
      "|           20|\n",
      "|           21|\n",
      "|           22|\n",
      "|           23|\n",
      "|           24|\n",
      "|           25|\n",
      "|           26|\n",
      "|           27|\n",
      "|           28|\n",
      "|           29|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ex\n",
    "df = spark.range(500).toDF('number')\n",
    "df.select(df['number']+10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스파크에서 덧셈 연산이 수행되는 이유는 스파크가 지원하는 언어를 이용해 작성된 표현식을 카탈리스트 엔진에서 스파크이 데이터 타입으로 변환해 명령을 처리하기 때문이다. \n",
    "\n",
    "### 4.3.1 데이터프레임과 데이터셋 비교\n",
    "- DataFrame = 비타입형 파이썬에서는 dataset을 dataframe으로 처리해야한다. \n",
    "- Dataset = 타입형, 스칼라와 자바만 지원\n",
    "데이터프레임은 스키마에 명시된 데이터 타입의 일치 여부를 런타임이 되어서야 확인한다. 반면 데이터셋은 스키마에 명시된 데이터 타입의 일치 여부를 컴파일 타임에 확인한다. \n",
    "<br>\n",
    "스파크의 데이터프레임은 row 타입으로 구성된 데이터셋이다. row타입은 스파크가 사용하는 연산에 최적화된 인메모리 포맷의 내부적 표현 방식이다. 이것을 사용하면 가비지 컬렉션과 객체 초기화 부하가 있는 JVM 데이터 타입을 사용하는 대신 자체 데이터 포맷을 사용하기 때문에 효율적인 연산이 가능하다. \n",
    "\n",
    "### 4.3.2 컬럼 \n",
    "- 단순 데이터 타입: 정수형이나 문자열\n",
    "- 복합 데이터 타입: 배열이나 맵\n",
    "- null\n",
    "\n",
    "### 4.3.3 로우\n",
    "로우는 데이터 레코드. \n",
    "\n",
    "## 4.4 구조적 API의 실행과정\n",
    "1. DataFrame/Dataset/SQL을 이용해 코드를 작성한다.\n",
    "2. 정상적인 코드라면 스파크가 논리적 실행 계획으로 변환한다.\n",
    "3. 스파크는 논리적 실행 계획을 물리적 실행 계획으로 변환하며 그 과정에서 추가적인 최적화를 할 수 있는지 확인한다.\n",
    "4. 스파크는 클러스터에서 물리적 실행 계획(RDD 처리)을 실행한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 구조적 API 기본 연산\n",
    "- DataFrame은 레코드와 컬럼으로 구성된다. \n",
    "- 스키마는 각 컬럼명과 데이터 타입을 정의한다. \n",
    "- 파티셔닝은 DataFrame/Dataset이 클러스터에서 물리적으로 배치되는 형태. \n",
    "- 파티셔닝 스키마는 파티션을 배치하는 방법을 정의한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('json').load(\"file:///home/ubuntu/structure api/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 스키마\n",
    "스키마는 데이터프레임의 칼럼명과 데이터 타입을 정의한다. 데이터소스에서 스키마를 얻거나 직접 정의할 수 있다.\n",
    "- 스키마는 여러 개의 structfield 타입 필드로 구성된 structtype 객체이다. \n",
    "- structfield는 이름, 데이터 타입, 컬럼이 값이 없거나 Null 일 수 있는지 지정하는 불리언 값을 가진다. \n",
    "- 필요한 경우 메타데이터를 지정할 수 있다. \n",
    "- 복합 데이터 타입인 structtype을 가질 수 있다. \n",
    "- 스파크는 런타임에 데이터 타입이 스키마의 데이터 타입과 일치하지 않으면 오류를 발생시킨다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('json').load(\"file:///home/ubuntu/structure api/2015-summary.json\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "mySchema = StructType([\n",
    "    StructField('DEST_COUNTRY_NAME', StringType(), True),\n",
    "    StructField('ORIGIN_COUNTRY_NAME', StringType(), True),\n",
    "    StructField('count', LongType(), False, metadata = {'hello': 'world'})\n",
    "])\n",
    "df = spark.read.format('json').schema(mySchema) \\\n",
    ".load('file:///home/ubuntu/structure api/2015-summary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 컬럼과 표현식\n",
    "스파크의 컬럼은 pandas의 dataframe 컬럼과 비슷하다. 표현식으로 DataFrame의 컬럼을 선택, 조작, 제거할 수 있다. \n",
    "\n",
    "### 5.2.1 컬럼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'SomeColumnName'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 컬럼은 컬럼명을 카탈로그에 저장된 정보와 비교하기 전까지 미확인 상태로 남는다.\n",
    "# 분석기가 동작하는 단계에서 컬럼과 테이블을 분석한다. \n",
    "# col은 조인 시 유용하다. \n",
    "from pyspark.sql.functions import col, column\n",
    "col('SomeColumnName')\n",
    "column('SomeColumnName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 표현식\n",
    "표현식은 DataFrame 레코드의 여러 값에 대한 트랜스포메이션 집합을 의미한다. \n",
    "- 표현식은 expr 함수로 간단하게 사용할 수 있다. \n",
    "- 컬럼은 단지 표현식일 뿐이다.\n",
    "- 컬럼과 컬럼의 트랜스포메이션은 파싱된 표현식과 동일한 논리적 실행 계획으로 컴파일된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'((((someCol + 5) * 200) - 6) < otherCol)'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL문에 추가해도 잘 사용할 수 있고 동일한 결과가 나온다. 왜냐하면 실행 시점에 동일한 논리 트리로 컴파일 되기 때문.\n",
    "from pyspark.sql.functions import expr\n",
    "expr(\"(((someCol + 5)*200)-6)<otherCol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 레코드와 로우\n",
    "- 스파크는 레코드를 row 객체로 표현하고, 값을 생성하기 위해 컬럼 표현식으로 row 객체를 다룬다. \n",
    "- row 객체는 내부에 바이트 배열을 가진다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 로우 생성하기\n",
    "row 객체는 스키마 정보를 가지고 있지 않고, dataFrame만 유일하게 스키마를 가진다. 따라서 row객체를 직접 생성하려면 DataFrame의 스키마와 같은 순서로 값을 명시해야한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "myRow = Row('Hello', None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(myRow[0])\n",
    "print(myRow[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 DataFrame의 트랜스포메이션\n",
    "DataFrame을 다루는 몇 가지 주요 작업\n",
    "- 로우나 칼럼 추가\n",
    "- 로우나 칼럼 제거\n",
    "- 로우를 컬럼으로 변환하거나, 그 반대로 변환\n",
    "- 컬럼값을 기준으로 로우 순서 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 DataFrame 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('json').load(\"file:///home/ubuntu/structure api/2015-summary.json\")\n",
    "df.createOrReplaceTempView('dfTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| Some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|null|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# row객체를 가진 seq 타입을 직접 변환해 DataFrame을 생성할 수 있다. \n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "mySchema = StructType([\n",
    "    StructField('Some', StringType(), True),\n",
    "    StructField('col', StringType(), True),\n",
    "    StructField('names', LongType(), False)\n",
    "])\n",
    "\n",
    "myRow = Row('Hello', None, 1)\n",
    "myDf = spark.createDataFrame([myRow], mySchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용하기 유용한 methods\n",
    "- 칼럼이나 표현식을 사용하는 select 메서드\n",
    "- 문자열 표현식을 사용하는 selectExpr 메서드\n",
    "- 메서드로 사용할 수 없는 org.apache.spark.sql.functions 패키지에 포함된 다양한 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 select와 selectExpr\n",
    "이 두개를 사용하면 테이블에 SQL을 실행하는 것처럼 DataFrame에도 실행 가능하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('DEST_COUNTRY_NAME').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 여러 컬럼 선택\n",
    "df.select('DEST_COUNTRY_NAME','ORIGIN_COUNTRY_NAME').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-------------------+-----------------+\n",
      "|    United States|            Romania|    United States|\n",
      "|    United States|            Croatia|    United States|\n",
      "+-----------------+-------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "df.select(expr('DEST_COUNTRY_NAME'),\n",
    "         col('ORIGIN_COUNTRY_NAME'),\n",
    "         column('DEST_COUNTRY_NAME')) \\\n",
    ".show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n",
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# expr을 이용해서 컬럼을 참조할 수 있다.\n",
    "print(df.select(expr('DEST_COUNTRY_NAME as destination' )).show(2))\n",
    "print(df.select(expr('DEST_COUNTRY_NAME as destination').alias('DEST_COUNTRY_NAME')).show(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selectExpr 메서드는 새로운 DataFrame을 생성하는 복잡한 표현식을 간단하게 만드는 도구다. 모든 유효한 비집계형 SQL 구문을 지정할 수 있다. 단 칼럼을 식별할 수 있어야한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('*',\n",
    "             '(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 집계함수를 지정할 수 있다.\n",
    "df.selectExpr('avg(count)', 'count(distinct(DEST_COUNTRY_NAME))').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.3 스파크 데이터 타입으로 변환하기\n",
    "가끔은 새로운 컬럼이 아닌 명시적인 값을 스파크에 전달해야되는 경우가 있다. 리터럴이라는 것을 사용하게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(expr('*'), lit(1).alias('One')).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.4 컬럼 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"numberOne\",lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('withinCountry', expr('ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count', 'Destination']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn('Destination', expr('DEST_COUNTRY_NAME')).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.5 컬럼명 변경하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dsst', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumnRenamed('DEST_COUNTRY_NAME','dsst').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.6 예약 문자와 키워드\n",
    "예약 문자를 컬럼명에 사용하려면 `을 사용하면 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwithLongColName = df.withColumn('This long column-name',\n",
    "                                 expr('origin_country_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|this long column-name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "+---------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfwithLongColName.selectExpr('`this long column-name`',\n",
    "                            '`this long column-name` as `new col`').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwithLongColName.createOrReplaceTempView('dfTableLong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this long column-name']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 표현식 대신 문자열을 사용해 명시적으로 컬럼을 참조하면 리터럴로 해석되기 때문에 예약 문자가 포함된 컬럼을 참조할 수 있다.\n",
    "dfwithLongColName.select(expr('`this long column-name`')).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.7 대소문자 구분\n",
    "대소문자 구분 따로 안해줘도 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.8 컬럼 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('origin_country_name').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count', 'This long column-name']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfwithLongColName.drop('origin_country_name','dest_country_name').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.9 컬럼의 데이터 타입 변경하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, count2: string]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn('count2',col('count').cast('string'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.10 로우 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.filter(col('count') < 2).show(2))\n",
    "print(df.where('count < 2').show(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# and 필터를 만들 때는 차례대로 필터를 연결하고 판단은 스파크에게 맡긴다. \n",
    "df.where(col('count') < 2).where(col('ORIGIN_COUNTRY_NAME')!= 'Croatia').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.11 고유한 로우 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "print(df.select('origin_country_name', 'dest_country_name').distinct().count())\n",
    "print(df.select('origin_country_name').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.12 무작위 샘플 만들기\n",
    "복원 추출이나 비복원 추출의 사용 여부를 지정할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement,fraction,seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.13 임의 분할하기\n",
    "머신러닝할 떄 트레인 테스트 셋으로 나눌 때 유용하다. 임의성을 가지도록 설계됐기 때문에 시드값은 반드시 설정해줘야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrames = df.randomSplit([0.25,0.75],seed)\n",
    "dataFrames[0].count() > dataFrames[1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.14 로우 합치기와 추가\n",
    "데이터프레임은 불변성을 가지고 있기 때문에 레코드 추가를 하는 작업은 불가능하다. 레코드 추가를 하려면 원본과 새로운 데이터프레임과 통합을 해야한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "schema = df.schema\n",
    "newRows = [\n",
    "    Row(\"New Country\", \"Other Country\", 5),\n",
    "    Row(\"New New Country\", \"Other Country\", 1)\n",
    "]\n",
    "\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|      New Country|      Other Country|    5|\n",
      "|  New New Country|      Other Country|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.15 로우 정렬하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('count').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy('count','dest_country_name').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col('count'),col('dest_country_name')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "|           Canada|      United States|  8399|\n",
      "|    United States|             Mexico|  7187|\n",
      "|           Mexico|      United States|  7140|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 오름차순, 내림차순 정렬도 가능하다. \n",
    "\n",
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "print(df.orderBy(expr('count desc')).show(5))\n",
    "print(df.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.16 로우 수 제한하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(expr('count desc')).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.17 repartition과 coalesce\n",
    "자주 필터링하는 컬럼을 기준으로 데이터를 분할하는 것도 최적화 기법이다. 이를 통해 파티셔닝 스키마와 파티션 수를 포함해 클러스터 전반의 물리적 데이터 구성을 제어할 수 있다. 향후에 사용할 파티션 수가 현재 파티션 수보다 많거나 컬럼을 기준으로 파티션을 만드는 경우에만 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repartition 메서드를 호출하면 무조건 전체 데이터를 셔플한다. \n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Denmark|  152|\n",
      "|       United States|         Martinique|   43|\n",
      "|       United States|        Saint Lucia|  136|\n",
      "|             Ireland|      United States|  335|\n",
      "|         South Korea|      United States| 1048|\n",
      "|       United States|              Italy|  438|\n",
      "|       United States|             Greece|   23|\n",
      "|Bonaire, Sint Eus...|      United States|   58|\n",
      "|              France|      United States|  935|\n",
      "|       United States|             Cyprus|    1|\n",
      "|       United States|         Montenegro|    1|\n",
      "|       United States|            Austria|   63|\n",
      "|           Australia|      United States|  329|\n",
      "|       United States|        New Zealand|   74|\n",
      "|       United States|           Suriname|   34|\n",
      "|       United States|           Malaysia|    3|\n",
      "|       United States|             Guyana|   63|\n",
      "|              Taiwan|      United States|  266|\n",
      "|            Portugal|      United States|  127|\n",
      "|             Bolivia|      United States|   30|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.repartition(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|            Anguilla|      United States|   41|\n",
      "|              Russia|      United States|  176|\n",
      "|            Paraguay|      United States|   60|\n",
      "|             Senegal|      United States|   40|\n",
      "|              Sweden|      United States|  118|\n",
      "|            Kiribati|      United States|   26|\n",
      "|              Guyana|      United States|   64|\n",
      "|         Philippines|      United States|  134|\n",
      "|            Djibouti|      United States|    1|\n",
      "|            Malaysia|      United States|    2|\n",
      "|           Singapore|      United States|    3|\n",
      "|                Fiji|      United States|   24|\n",
      "|              Turkey|      United States|  138|\n",
      "|                Iraq|      United States|    1|\n",
      "|             Germany|      United States| 1468|\n",
      "|              Jordan|      United States|   44|\n",
      "|               Palau|      United States|   30|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|              France|      United States|  935|\n",
      "|              Greece|      United States|   30|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 자주 필터링되는 컬럼을 기준으로 파티션을 재분배하는 것이 좋다.\n",
    "df.repartition(col('dest_country_name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|             Moldova|      United States|    1|\n",
      "|             Bolivia|      United States|   30|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|            Pakistan|      United States|   12|\n",
      "|    Marshall Islands|      United States|   42|\n",
      "|            Suriname|      United States|    1|\n",
      "|              Panama|      United States|  510|\n",
      "|         New Zealand|      United States|  111|\n",
      "|             Liberia|      United States|    2|\n",
      "|             Ireland|      United States|  335|\n",
      "|              Zambia|      United States|    1|\n",
      "|            Malaysia|      United States|    2|\n",
      "|               Japan|      United States| 1548|\n",
      "|    French Polynesia|      United States|   43|\n",
      "|           Singapore|      United States|    3|\n",
      "|             Denmark|      United States|  153|\n",
      "|               Spain|      United States|  420|\n",
      "|             Bermuda|      United States|  183|\n",
      "|            Kiribati|      United States|   26|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 선택적으로 파티션 수 지정\n",
    "df.repartition(5, col('DEST_country_name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coalesce는 전체 데이터를 셔플하지 않고 파티션을 병합하려는 경우에 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|             Moldova|      United States|    1|\n",
      "|             Bolivia|      United States|   30|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|            Pakistan|      United States|   12|\n",
      "|    Marshall Islands|      United States|   42|\n",
      "|            Suriname|      United States|    1|\n",
      "|              Panama|      United States|  510|\n",
      "|         New Zealand|      United States|  111|\n",
      "|             Liberia|      United States|    2|\n",
      "|             Ireland|      United States|  335|\n",
      "|              Zambia|      United States|    1|\n",
      "|            Malaysia|      United States|    2|\n",
      "|               Japan|      United States| 1548|\n",
      "|    French Polynesia|      United States|   43|\n",
      "|           Singapore|      United States|    3|\n",
      "|             Denmark|      United States|  153|\n",
      "|               Spain|      United States|  420|\n",
      "|             Bermuda|      United States|  183|\n",
      "|            Kiribati|      United States|   26|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.repartition(5, col('DEST_country_name')).coalesce(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
